import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import pandas as pd
from docx import Document

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer



doc = Document('speech.docx')
content = '\n'.join([para.text for para in doc.paragraphs])

speeches = []
lines = content.strip().split('\n')

i = 0
speech_count = 0
while i < len(lines):
    line = lines[i].strip()
    
    if line == '#':
        speech_count += 1
        
        if i + 4 < len(lines):
            line4 = lines[i+4]
            speaker_part = ""
            keywords_part = ""
            
            if ':' in line4:
                parts = line4.split(':', 1)
                speaker_part = parts[0].strip()
                if len(parts) > 1:
                    keywords_part = parts[1].strip()
            else:
                speaker_part = line4.strip()
            
            speech = {
                'id': speech_count,
                'title': lines[i+1].strip(),
                'date': lines[i+2].strip(),
                'url': lines[i+3].strip(),
                'speaker': speaker_part,
                'keywords': keywords_part,  # KEYWORDS ADDED
                'text': ''
            }
            
            text_lines = []
            j = i + 5
            while j < len(lines) and not (lines[j].strip().startswith('#')):
                clean_line = lines[j].strip()
                text_lines.append(clean_line)
                j += 1
            
            speech['text'] = ' '.join(text_lines)
            speeches.append(speech)
            i = j
        else:
            i += 1
    else:
        i += 1

df = pd.DataFrame(speeches)

if len(df) > 0:
    df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))
    df['char_count'] = df['text'].apply(lambda x: len(str(x)))
    
    df.to_csv('speech.csv', index=False, encoding='utf-8')
    
    try:
        with pd.ExcelWriter('speech.xlsx', engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Speeches', index=False)
            
            workbook = writer.book
            worksheet = writer.sheets['Speeches']
            
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
            
    except Exception as e:
        print("Excel save error: {e}")
else:
    print("No speeches found!")

print(f"\nğŸ Process completed!")

df = pd.read_csv('speech.csv')

df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['year_month'] = df['date'].dt.to_period('M')

print(f"Toplam konuÅŸma sayÄ±sÄ±: {len(df)}")

print("\nÄ°lk 5 konuÅŸma:")
print(df[['date', 'title', 'word_count']].head())
print(f"\nTarih aralÄ±ÄŸÄ±: {df['date'].min()} - {df['date'].max()}")

# 4. Temel istatistikler
print("\nKelime sayÄ±sÄ± istatistikleri:")
print(df['word_count'].describe())

# 5. Zaman daÄŸÄ±lÄ±mÄ± grafiÄŸi
plt.figure(figsize=(12, 5))
df.groupby('year_month').size().plot(kind='bar', color='steelblue')
plt.title('KonuÅŸmalarÄ±n Zaman Ä°Ã§inde DaÄŸÄ±lÄ±mÄ±')
plt.xlabel('Ay')
plt.ylabel('KonuÅŸma SayÄ±sÄ±')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# -*- coding: utf-8 -*-
"""
SMART STOPWORDS DETECTOR - OPTIMIZED VERSION
Putin KonuÅŸmalarÄ± iÃ§in AkÄ±llÄ± Stopwords Tespit Sistemi
Hedef: %30-40 optimal azalma oranÄ±
"""

import pandas as pd
import numpy as np
import re
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import os
import warnings
warnings.filterwarnings('ignore')

class SmartStopwordsDetectorOptimized:
    """
    Optimize edilmiÅŸ akÄ±llÄ± stopwords tespit sÄ±nÄ±fÄ±
    Putin konuÅŸmalarÄ± iÃ§in Ã¶zel olarak tasarlandÄ±
    """
    
    def __init__(self, csv_path, text_column='text', target_reduction=35):
        """
        Parameters:
        -----------
        csv_path : str
            CSV dosya yolu
        text_column : str
            Metin sÃ¼tunu adÄ±
        target_reduction : int
            Hedeflenen azalma yÃ¼zdesi (30-40 arasÄ± optimal)
        """
        print("="*60)
        print("ğŸ¤– SMART STOPWORDS DETECTOR - OPTIMIZED")
        print("="*60)
        
        # Parametreler
        self.target_reduction = target_reduction
        self.text_column = text_column
        
        # Veriyi yÃ¼kle
        print(f"\nğŸ“‚ Veri yÃ¼kleniyor: {csv_path}")
        self.df = pd.read_csv(csv_path)
        print(f"   âœ“ YÃ¼klenen satÄ±r: {len(self.df)}")
        
        # Metinleri hazÄ±rla
        self.prepare_texts()
        
        # SonuÃ§lar
        self.stopwords_results = {}
        self.final_stopwords = []
        
    def prepare_texts(self):
        """Metinleri temizle ve hazÄ±rla"""
        print("\nğŸ§¹ Metinler temizleniyor...")
        
        # NaN kontrolÃ¼
        self.df[self.text_column] = self.df[self.text_column].fillna('')
        
        # GeliÅŸmiÅŸ temizleme fonksiyonu
        def advanced_clean(text):
            text = str(text).lower()
            
            # Noktalama ve Ã¶zel karakterler
            text = re.sub(r'[^\w\s]', ' ', text)
            
            # SayÄ±lar
            text = re.sub(r'\d+', '', text)
            
            # Fazla boÅŸluklar
            text = re.sub(r'\s+', ' ', text).strip()
            
            return text
        
        # Temizleme uygula
        self.df['cleaned_text'] = self.df[self.text_column].apply(advanced_clean)
        self.texts = self.df['cleaned_text'].tolist()
        
        # Ä°statistikler
        total_words = sum(len(text.split()) for text in self.texts)
        unique_words = len(set(' '.join(self.texts).split()))
        
        print(f"   âœ“ TemizlenmiÅŸ konuÅŸma: {len(self.texts)}")
        print(f"   âœ“ Toplam kelime: {total_words:,}")
        print(f"   âœ“ Benzersiz kelime: {unique_words:,}")
    
    def get_core_english_stopwords(self):
        """Ã‡ekirdek Ä°ngilizce stopwords listesi"""
        return {
            # Articles
            'a', 'an', 'the',
            
            # Common pronouns
            'i', 'you', 'he', 'she', 'it', 'we', 'they',
            'me', 'him', 'her', 'us', 'them',
            'my', 'your', 'his', 'her', 'its', 'our', 'their',
            'mine', 'yours', 'hers', 'ours', 'theirs',
            
            # Common prepositions
            'in', 'on', 'at', 'by', 'for', 'with', 'about', 'against',
            'between', 'into', 'through', 'during', 'before', 'after',
            'above', 'below', 'to', 'from', 'up', 'down', 'out', 'off',
            'over', 'under', 'again', 'further',
            
            # Common conjunctions
            'and', 'but', 'or', 'nor', 'so', 'yet',
            'although', 'because', 'since', 'unless',
            
            # Common verbs (to be, to have, to do)
            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'having',
            'do', 'does', 'did', 'doing',
            
            # Common adverbs
            'very', 'really', 'quite', 'just', 'only', 'also',
            'well', 'too', 'even', 'still', 'always', 'never',
            
            # Common determiners
            'this', 'that', 'these', 'those',
            'all', 'any', 'both', 'each', 'few', 'more', 'most',
            'other', 'some', 'such', 'no', 'nor', 'not', 'only',
            'own', 'same', 'so', 'than', 'too',
            
            # Question words
            'what', 'which', 'who', 'whom', 'whose',
            'when', 'where', 'why', 'how',
            
            # Modal verbs
            'will', 'would', 'can', 'could', 'shall', 'should',
            'may', 'might', 'must'
        }
    
    def get_putin_context_stopwords(self):
        """Putin konuÅŸmalarÄ± baÄŸlamÄ±na Ã¶zel stopwords"""
        return {
            # Putin'e Ã¶zel fiiller ve yardÄ±mcÄ± fiiller
            'said', 'says', 'according', 'regarding', 'including',
            'within', 'without', 'upon', 'among', 'through',
            
            # Siyasi terminoloji (genel)
            'country', 'countries', 'state', 'states',
            'government', 'governments', 'people', 'peoples',
            'nation', 'national', 'international',
            
            # Zaman ifadeleri (Ã§ok genel)
            'today', 'yesterday', 'tomorrow', 'now', 'then',
            'year', 'years', 'month', 'months', 'day', 'days',
            'time', 'times', 'period', 'periods',
            
            # Miktar ifadeleri (Ã§ok genel)
            'many', 'much', 'more', 'most', 'several', 'various',
            
            # CoÄŸrafi terimler (analize baÄŸlÄ± - opsiyonel)
            # 'russia', 'russian', 'ukraine', 'ukrainian',
            # 'moscow', 'kyiv', 'kiev'
        }
    
    def analyze_statistical_methods(self):
        """Ä°statistiksel yÃ¶ntemlerle stopwords tespiti"""
        print("\n" + "="*60)
        print("ğŸ“Š Ä°STATÄ°STÄ°KSEL ANALÄ°Z YÃ–NTEMLERÄ°")
        print("="*60)
        
        # TÃ¼m kelimeleri say
        all_words = ' '.join(self.texts).split()
        word_counts = Counter(all_words)
        total_words = len(all_words)
        
        # DokÃ¼man frekanslarÄ±
        doc_freq = defaultdict(int)
        for text in self.texts:
            for word in set(text.split()):
                doc_freq[word] += 1
        
        N = len(self.texts)
        
        print(f"\nğŸ“ˆ TEMEL Ä°STATÄ°STÄ°KLER:")
        print(f"   Toplam kelime: {total_words:,}")
        print(f"   Benzersiz kelime: {len(word_counts):,}")
        print(f"   Ortalama dokÃ¼man uzunluÄŸu: {total_words/N:.0f} kelime")
        
        # 1. YÃœKSEK FREKANSLI KELÄ°MELER ANALÄ°ZÄ°
        print("\nğŸ” YÃœKSEK FREKANSLI KELÄ°MELER ANALÄ°ZÄ°:")
        
        high_freq_words = []
        top_50 = word_counts.most_common(50)
        
        for word, freq in top_50:
            word_pct = (freq / total_words) * 100
            doc_pct = (doc_freq[word] / N) * 100
            
            # Kritik eÅŸikler
            if word_pct > 0.1:  # %0.1'den fazla
                high_freq_words.append((word, freq, word_pct, doc_pct))
                status = "ğŸš¨ YÃœKSEK" if word_pct > 0.5 else "âš ï¸ ORTA"
                print(f"   {status} {word:12} â†’ %{word_pct:.2f} (dokÃ¼man: %{doc_pct:.1f})")
        
        self.stopwords_results['high_freq'] = {w[0] for w in high_freq_words}
        
        # 2. YÃœKSEK DOKÃœMAN FREKANSI ANALÄ°ZÄ°
        print("\nğŸ“„ YÃœKSEK DOKÃœMAN FREKANSI ANALÄ°ZÄ°:")
        
        high_doc_words = []
        for word, df in doc_freq.items():
            doc_pct = (df / N) * 100
            if doc_pct > 50:  # %50'den fazla dokÃ¼manda
                freq = word_counts[word]
                word_pct = (freq / total_words) * 100
                high_doc_words.append((word, freq, word_pct, doc_pct))
        
        # SÄ±rala ve gÃ¶ster
        high_doc_words.sort(key=lambda x: x[3], reverse=True)
        for word, freq, word_pct, doc_pct in high_doc_words[:20]:
            print(f"   ğŸ“„ {word:12} â†’ %{doc_pct:.1f} dokÃ¼manda (frekans: %{word_pct:.2f})")
        
        self.stopwords_results['high_doc'] = {w[0] for w in high_doc_words}
        
        # 3. TF-IDF ANALÄ°ZÄ° (DÃœÅÃœK SKORLU KELÄ°MELER)
        print("\nğŸ¤– TF-IDF ANALÄ°ZÄ° (DÃ¼ÅŸÃ¼k Ã–nemli Kelimeler):")
        
        tfidf_stopwords = self.tfidf_analysis()
        self.stopwords_results['low_tfidf'] = tfidf_stopwords
        
        # 4. KISA KELÄ°MELER ANALÄ°ZÄ°
        print("\nğŸ”¤ KISA KELÄ°MELER ANALÄ°ZÄ°:")
        short_words = {w for w in word_counts if len(w) <= 2}
        self.stopwords_results['short_words'] = short_words
        
        # KÄ±sa kelimelerin etkisi
        short_impact = sum(word_counts[w] for w in short_words)
        print(f"   âœ“ {len(short_words)} kÄ±sa kelime")
        print(f"   âœ“ Toplam kullanÄ±m: {short_impact:,} (%{(short_impact/total_words)*100:.1f})")
        
        return self.stopwords_results
    
    def tfidf_analysis(self, low_percentile=20):
        """TF-IDF ile dÃ¼ÅŸÃ¼k Ã¶nemli kelimeleri bul"""
        try:
            vectorizer = TfidfVectorizer(
                max_features=2000,
                min_df=2,
                max_df=0.85,
                stop_words='english'
            )
            
            tfidf_matrix = vectorizer.fit_transform(self.texts)
            features = vectorizer.get_feature_names_out()
            scores = np.array(tfidf_matrix.mean(axis=0)).flatten()
            
            # Normalize scores
            if scores.max() > scores.min():
                norm_scores = (scores - scores.min()) / (scores.max() - scores.min())
            else:
                norm_scores = scores
            
            # Low percentile threshold
            threshold = np.percentile(norm_scores, low_percentile)
            
            # Low TF-IDF words
            low_tfidf_words = {word for word, score in zip(features, norm_scores) 
                              if score <= threshold}
            
            # Show top low TF-IDF words
            word_scores = [(word, score) for word, score in zip(features, norm_scores) 
                          if score <= threshold]
            word_scores.sort(key=lambda x: x[1])
            
            print(f"   ğŸ¯ {len(low_tfidf_words)} dÃ¼ÅŸÃ¼k TF-IDF kelime")
            print(f"   ğŸ“Š EÅŸik deÄŸeri: {threshold:.4f}")
            
            if word_scores:
                print(f"   ğŸ“‹ Ã–rnekler: {', '.join([w[0] for w in word_scores[:10]])}")
            
            return low_tfidf_words
            
        except Exception as e:
            print(f"   âš ï¸ TF-IDF hatasÄ±: {e}")
            return set()
    
    def ensemble_voting(self, min_votes=2):
        """Ensemble yÃ¶ntemi ile birleÅŸik stopwords belirle"""
        print("\n" + "="*60)
        print("ğŸ¤ ENSEMBLE OYLAMA SÄ°STEMÄ°")
        print("="*60)
        
        # Oylama sistemi
        votes = defaultdict(int)
        all_words_set = set()
        
        for method, words in self.stopwords_results.items():
            for word in words:
                votes[word] += 1
                all_words_set.add(word)
        
        # Oylama sonuÃ§larÄ±nÄ± analiz et
        print(f"\nğŸ“Š OY DAÄILIMI:")
        vote_distribution = defaultdict(int)
        for word, vote_count in votes.items():
            vote_distribution[vote_count] += 1
        
        for vote_count in sorted(vote_distribution.keys()):
            count = vote_distribution[vote_count]
            print(f"   {vote_count} yÃ¶ntem tarafÄ±ndan seÃ§ilen: {count} kelime")
        
        # Ã‡oÄŸunluk oyu ile stopwords belirle
        majority_stopwords = {word for word, vote_count in votes.items() 
                             if vote_count >= min_votes}
        
        print(f"\nâœ… ENSEMBLE SONUÃ‡LARI:")
        print(f"   Toplam aday: {len(all_words_set)}")
        print(f"   Ã‡oÄŸunluk ({min_votes}+ oy): {len(majority_stopwords)}")
        
        return majority_stopwords
    
    def optimize_for_target(self, candidate_stopwords):
        """Hedef azalma oranÄ±na gÃ¶re stopwords optimizasyonu"""
        print("\n" + "="*60)
        print(f"ğŸ¯ HEDEF OPTÄ°MÄ°ZASYONU: %{self.target_reduction} AZALMA")
        print("="*60)
        
        # Kelime frekanslarÄ±
        all_words = ' '.join(self.texts).split()
        word_counts = Counter(all_words)
        total_words = len(all_words)
        
        # Aday stopwords'leri frekansa gÃ¶re sÄ±rala
        candidate_freq = [(w, word_counts.get(w, 0)) for w in candidate_stopwords 
                         if w in word_counts]
        candidate_freq.sort(key=lambda x: x[1], reverse=True)
        
        # Hedef frekansÄ± hesapla
        target_freq = total_words * (self.target_reduction / 100)
        
        # Optimal stopwords'leri seÃ§
        optimal_stopwords = []
        accumulated_freq = 0
        
        for word, freq in candidate_freq:
            if accumulated_freq + freq <= target_freq:
                optimal_stopwords.append(word)
                accumulated_freq += freq
            else:
                # Hedefe Ã§ok yakÄ±nsa ekle
                if (target_freq - accumulated_freq) / target_freq > 0.1:
                    optimal_stopwords.append(word)
                    accumulated_freq += freq
        
        # Coverage hesapla
        coverage = (accumulated_freq / total_words) * 100
        
        print(f"\nğŸ“ˆ OPTÄ°MÄ°ZASYON SONUÃ‡LARI:")
        print(f"   BaÅŸlangÄ±Ã§ aday: {len(candidate_freq)}")
        print(f"   SeÃ§ilen stopwords: {len(optimal_stopwords)}")
        print(f"   Hedef frekans: {target_freq:,.0f}")
        print(f"   GerÃ§ekleÅŸen: {accumulated_freq:,.0f}")
        print(f"   KAPSAMA ORANI: %{coverage:.1f}")
        
        # Ä°deal aralÄ±k kontrolÃ¼
        if 25 <= coverage <= 45:
            print(f"   âœ… OPTÄ°MAL ARALIKTA (%25-%45)")
        elif coverage < 25:
            print(f"   âš ï¸ DÃœÅÃœK KAPSAMA, daha agresif filtreleme gerekebilir")
        else:
            print(f"   âš ï¸ YÃœKSEK KAPSAMA, daha seÃ§ici filtreleme gerekebilir")
        
        return optimal_stopwords
    
    def get_final_stopwords(self):
        """Nihai stopwords listesini oluÅŸtur"""
        print("\n" + "="*60)
        print("ğŸ NÄ°HAÄ° STOPWORDS BELÄ°RLENÄ°YOR")
        print("="*60)
        
        # 1. Ä°statistiksel analizleri Ã§alÄ±ÅŸtÄ±r
        self.analyze_statistical_methods()
        
        # 2. Ensemble yÃ¶ntemi ile adaylarÄ± belirle
        candidate_stopwords = self.ensemble_voting(min_votes=2)
        
        # 3. Ã‡ekirdek stopwords ekle
        core_stopwords = self.get_core_english_stopwords()
        context_stopwords = self.get_putin_context_stopwords()
        
        # 4. BirleÅŸtir
        all_candidates = candidate_stopwords | core_stopwords | context_stopwords
        
        # 5. Hedef optimizasyonu
        self.final_stopwords = self.optimize_for_target(all_candidates)
        
        # 6. Ä°statistikleri gÃ¶ster
        self.show_final_statistics()
        
        return self.final_stopwords
    
    def show_final_statistics(self):
        """Nihai istatistikleri gÃ¶ster"""
        # Kelime frekanslarÄ±
        all_words = ' '.join(self.texts).split()
        word_counts = Counter(all_words)
        total_words = len(all_words)
        
        # Stopwords istatistikleri
        stopwords_set = set(self.final_stopwords)
        stopwords_freq = sum(word_counts.get(w, 0) for w in stopwords_set)
        coverage = (stopwords_freq / total_words) * 100
        
        print(f"\nğŸ“Š NÄ°HAÄ° Ä°STATÄ°STÄ°KLER:")
        print(f"   Stopwords sayÄ±sÄ±: {len(self.final_stopwords)}")
        print(f"   Kapsanan kelime: {stopwords_freq:,}")
        print(f"   Toplam kelime: {total_words:,}")
        print(f"   KAPSAMA ORANI: %{coverage:.1f}")
        
        print(f"\nğŸ† EN ETKÄ°LÄ° 25 STOPWORDS:")
        print("-" * 60)
        
        # Stopwords'leri frekansa gÃ¶re sÄ±rala
        stopword_stats = []
        for word in self.final_stopwords:
            freq = word_counts.get(word, 0)
            if freq > 0:
                pct = (freq / total_words) * 100
                stopword_stats.append((word, freq, pct))
        
        stopword_stats.sort(key=lambda x: x[1], reverse=True)
        
        for i, (word, freq, pct) in enumerate(stopword_stats[:25], 1):
            doc_count = sum(1 for text in self.texts if word in text)
            doc_pct = (doc_count / len(self.texts)) * 100
            print(f"{i:2}. {word:15} â†’ {freq:6,} kez (%{pct:.2f}) | %{doc_pct:.0f} dokÃ¼manda")
    
    def apply_stopwords_filter(self):
        """Stopwords'leri uygula ve sonuÃ§larÄ± gÃ¶ster"""
        print("\n" + "="*60)
        print("ğŸ”„ STOPWORDS FÄ°LTRELEME UYGULANIYOR")
        print("="*60)
        
        stopwords_set = set(self.final_stopwords)
        
        # Filtreleme fonksiyonu
        def filter_text(text):
            words = text.split()
            filtered = [w for w in words if w not in stopwords_set]
            return ' '.join(filtered)
        
        # Uygula
        self.df['filtered_text'] = self.df['cleaned_text'].apply(filter_text)
        
        # Ä°statistikler hesapla
        original_counts = [len(t.split()) for t in self.texts]
        filtered_counts = [len(t.split()) for t in self.df['filtered_text']]
        
        original_total = sum(original_counts)
        filtered_total = sum(filtered_counts)
        reduction_pct = ((original_total - filtered_total) / original_total) * 100
        
        print(f"\nğŸ“ˆ FÄ°LTRELEME SONUÃ‡LARI:")
        print(f"   Orijinal toplam: {original_total:,} kelime")
        print(f"   FiltrelenmiÅŸ: {filtered_total:,} kelime")
        print(f"   Ã‡Ä±karÄ±lan: {original_total - filtered_total:,} kelime")
        print(f"   AZALMA ORANI: %{reduction_pct:.1f}")
        
        # Ã–rnek karÅŸÄ±laÅŸtÄ±rma
        print(f"\nğŸ” Ã–RNEK KARÅILAÅTIRMA:")
        sample_idx = min(2, len(self.df) - 1)
        
        original_text = self.texts[sample_idx]
        filtered_text = self.df['filtered_text'].iloc[sample_idx]
        
        print(f"   KonuÅŸma #{sample_idx + 1}:")
        print(f"   Orijinal: {len(original_text.split())} kelime")
        print(f"   FiltrelenmiÅŸ: {len(filtered_text.split())} kelime")
        print(f"   Ã‡Ä±karÄ±lan: {len(original_text.split()) - len(filtered_text.split())} kelime")
        
        print(f"\n   FiltrelenmiÅŸ metin (ilk 250 karakter):")
        print(f"   \"{filtered_text[:250]}...\"")
        
        return reduction_pct
    
    def save_results(self, output_dir='smart_stopwords_results'):
        """SonuÃ§larÄ± kaydet"""
        print("\n" + "="*60)
        print("ğŸ’¾ SONUÃ‡LAR KAYDEDÄ°LÄ°YOR")
        print("="*60)
        
        # KlasÃ¶r oluÅŸtur
        os.makedirs(output_dir, exist_ok=True)
        
        # 1. Stopwords listesini kaydet
        stopwords_file = f'{output_dir}/smart_stopwords.txt'
        with open(stopwords_file, 'w', encoding='utf-8') as f:
            f.write("# SMART STOPWORDS DETECTOR - OPTIMIZED RESULTS\n")
            f.write(f"# Tarih: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# Toplam konuÅŸma: {len(self.texts)}\n")
            f.write(f"# Stopwords sayÄ±sÄ±: {len(self.final_stopwords)}\n")
            f.write(f"# Hedef azalma: %{self.target_reduction}\n\n")
            
            # Kelime frekanslarÄ±
            all_words = ' '.join(self.texts).split()
            word_counts = Counter(all_words)
            
            f.write("KELÄ°ME | FREKANS | YÃœZDE | DOKÃœMAN_YÃœZDESÄ°\n")
            f.write("-"*50 + "\n")
            
            for word in self.final_stopwords:
                freq = word_counts.get(word, 0)
                word_pct = (freq / len(all_words)) * 100 if len(all_words) > 0 else 0
                doc_count = sum(1 for text in self.texts if word in text)
                doc_pct = (doc_count / len(self.texts)) * 100
                
                f.write(f"{word:<20} | {freq:>8,} | %{word_pct:>5.2f} | %{doc_pct:>5.1f}\n")
        
        # 2. FiltrelenmiÅŸ veriyi kaydet
        filtered_file = f'{output_dir}/filtered_speeches.csv'
        self.df.to_csv(filtered_file, index=False, encoding='utf-8')
        
        # 3. Analiz raporu oluÅŸtur
        report_file = f'{output_dir}/analysis_report.txt'
        self.create_analysis_report(report_file)
        
        print(f"\nâœ… KAYDEDÄ°LEN DOSYALAR:")
        print(f"   ğŸ“„ {stopwords_file}")
        print(f"   ğŸ“„ {filtered_file}")
        print(f"   ğŸ“„ {report_file}")
        print(f"\n   ğŸ“ TÃ¼m sonuÃ§lar: {os.path.abspath(output_dir)}/")
    
    def create_analysis_report(self, report_file):
        """Analiz raporu oluÅŸtur"""
        all_words = ' '.join(self.texts).split()
        word_counts = Counter(all_words)
        total_words = len(all_words)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("SMART STOPWORDS DETECTOR - ANALÄ°Z RAPORU\n")
            f.write("="*70 + "\n\n")
            
            f.write(f"ANALÄ°Z TARÄ°HÄ°: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"VERÄ° SETÄ°: {len(self.texts)} konuÅŸma\n")
            f.write(f"TOPLAM KELÄ°ME: {total_words:,}\n")
            f.write(f"BENZERSÄ°Z KELÄ°ME: {len(word_counts):,}\n")
            f.write(f"HEDEF AZALMA: %{self.target_reduction}\n\n")
            
            f.write("\nSTOPWORDS ANALÄ°ZÄ°:\n")
            f.write("-"*40 + "\n")
            
            for method, words in self.stopwords_results.items():
                f.write(f"\n{method.upper()}:\n")
                f.write(f"  Kelime sayÄ±sÄ±: {len(words)}\n")
                
                # Ä°lk 10 kelime
                sorted_words = sorted(words, key=lambda x: word_counts.get(x, 0), reverse=True)
                f.write(f"  Ã–rnekler: {', '.join(list(sorted_words)[:10])}\n")
            
            f.write("\n" + "="*70 + "\n")
            f.write("NÄ°HAÄ° STOPWORDS Ä°STATÄ°STÄ°KLERÄ°:\n")
            f.write("-"*40 + "\n")
            
            stopwords_set = set(self.final_stopwords)
            stopwords_freq = sum(word_counts.get(w, 0) for w in stopwords_set)
            coverage = (stopwords_freq / total_words) * 100
            
            f.write(f"Toplam stopwords: {len(self.final_stopwords)}\n")
            f.write(f"Kapsanan kelime: {stopwords_freq:,}\n")
            f.write(f"Kapsama oranÄ±: %{coverage:.1f}\n\n")
            
            f.write("EN ETKÄ°LÄ° 50 STOPWORDS:\n")
            f.write("-"*40 + "\n")
            
            stopword_stats = []
            for word in self.final_stopwords:
                freq = word_counts.get(word, 0)
                if freq > 0:
                    pct = (freq / total_words) * 100
                    stopword_stats.append((word, freq, pct))
            
            stopword_stats.sort(key=lambda x: x[1], reverse=True)
            
            for i, (word, freq, pct) in enumerate(stopword_stats[:50], 1):
                doc_count = sum(1 for text in self.texts if word in text)
                doc_pct = (doc_count / len(self.texts)) * 100
                f.write(f"{i:3}. {word:<20} {freq:>8,} kez (%{pct:>5.2f}) | %{doc_pct:>5.1f} dokÃ¼man\n")
    
    def visualize_results(self):
        """SonuÃ§larÄ± gÃ¶rselleÅŸtir"""
        print("\n" + "="*60)
        print("ğŸ“Š GÃ–RSELLEÅTÄ°RME OLUÅTURULUYOR")
        print("="*60)
        
        try:
            # 1. Stopwords daÄŸÄ±lÄ±mÄ±
            fig, axes = plt.subplots(2, 2, figsize=(14, 10))
            fig.suptitle('Smart Stopwords Detector - Analiz SonuÃ§larÄ±', 
                        fontsize=16, fontweight='bold')
            
            # Veri hazÄ±rla
            all_words = ' '.join(self.texts).split()
            word_counts = Counter(all_words)
            stopwords_set = set(self.final_stopwords)
            
            # A. Stopwords frekans daÄŸÄ±lÄ±mÄ±
            ax1 = axes[0, 0]
            top_stopwords = sorted(stopwords_set, 
                                  key=lambda x: word_counts.get(x, 0), 
                                  reverse=True)[:15]
            top_freqs = [word_counts.get(w, 0) for w in top_stopwords]
            
            colors = plt.cm.Set3(np.linspace(0, 1, len(top_stopwords)))
            ax1.barh(range(len(top_stopwords)), top_freqs, color=colors)
            ax1.set_yticks(range(len(top_stopwords)))
            ax1.set_yticklabels(top_stopwords)
            ax1.set_xlabel('Frekans')
            ax1.set_title('En SÄ±k KullanÄ±lan 15 Stopwords')
            ax1.invert_yaxis()
            
            # B. YÃ¶ntemlere gÃ¶re stopwords sayÄ±sÄ±
            ax2 = axes[0, 1]
            methods = list(self.stopwords_results.keys())
            method_counts = [len(words) for words in self.stopwords_results.values()]
            
            bars = ax2.bar(methods, method_counts, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
            ax2.set_title('YÃ¶ntemlere GÃ¶re Stopwords SayÄ±sÄ±')
            ax2.set_ylabel('Kelime SayÄ±sÄ±')
            ax2.tick_params(axis='x', rotation=45)
            
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + 5,
                        f'{int(height)}', ha='center', va='bottom')
            
            # C. Filtreleme etkisi
            ax3 = axes[1, 0]
            original_counts = [len(t.split()) for t in self.texts[:5]]
            filtered_counts = [len(t.split()) for t in self.df['filtered_text'][:5]]
            
            x = range(len(original_counts))
            width = 0.35
            ax3.bar([i - width/2 for i in x], original_counts, width, 
                   label='Orijinal', color='gray', alpha=0.7)
            ax3.bar([i + width/2 for i in x], filtered_counts, width, 
                   label='FiltrelenmiÅŸ', color='lightblue', alpha=0.7)
            ax3.set_title('Ä°lk 5 KonuÅŸma - Filtreleme Etkisi')
            ax3.set_xlabel('KonuÅŸma No')
            ax3.set_ylabel('Kelime SayÄ±sÄ±')
            ax3.set_xticks(x)
            ax3.set_xticklabels([f'#{i+1}' for i in x])
            ax3.legend()
            
            # D. Stopwords uzunluk daÄŸÄ±lÄ±mÄ±
            ax4 = axes[1, 1]
            stopword_lengths = [len(w) for w in stopwords_set]
            length_counts = Counter(stopword_lengths)
            
            lengths = sorted(length_counts.keys())
            counts = [length_counts[l] for l in lengths]
            
            ax4.bar(lengths, counts, color='darkorange', alpha=0.7)
            ax4.set_title('Stopwords Uzunluk DaÄŸÄ±lÄ±mÄ±')
            ax4.set_xlabel('Kelime UzunluÄŸu')
            ax4.set_ylabel('Kelime SayÄ±sÄ±')
            
            plt.tight_layout()
            plt.show()
            
            print("âœ… GÃ¶rselleÅŸtirmeler oluÅŸturuldu")
            
        except Exception as e:
            print(f"âš ï¸ GÃ¶rselleÅŸtirme hatasÄ±: {e}")
    
    def run_full_analysis(self):
        """Tam analiz pipeline'Ä±nÄ± Ã§alÄ±ÅŸtÄ±r"""
        print("\n" + "="*60)
        print("ğŸš€ TAM ANALÄ°Z PIPELINE BAÅLATILIYOR")
        print("="*60)
        
        try:
            # 1. Stopwords belirle
            stopwords = self.get_final_stopwords()
            
            # 2. Filtrelemeyi uygula
            reduction = self.apply_stopwords_filter()
            
            # 3. GÃ¶rselleÅŸtir
            self.visualize_results()
            
            # 4. Kaydet
            self.save_results()
            
            # 5. SonuÃ§ Ã¶zeti
            print("\n" + "="*60)
            print("ğŸ‰ ANALÄ°Z BAÅARIYLA TAMAMLANDI!")
            print("="*60)
            print(f"âœ“ Toplam konuÅŸma: {len(self.texts)}")
            print(f"âœ“ Stopwords sayÄ±sÄ±: {len(stopwords)}")
            print(f"âœ“ Azalma oranÄ±: %{reduction:.1f}")
            print(f"âœ“ Hedef azalma: %{self.target_reduction}")
            
            if abs(reduction - self.target_reduction) <= 10:
                print(f"âœ“ âœ… HEDEFE YAKIN (fark: %{abs(reduction - self.target_reduction):.1f})")
            else:
                print(f"âœ“ âš ï¸ HEDEFTEN UZAK (fark: %{abs(reduction - self.target_reduction):.1f})")
            
            print(f"\nğŸ“ SonuÃ§lar: 'smart_stopwords_results/' klasÃ¶rÃ¼nde")
            
            return {
                'stopwords': stopwords,
                'reduction': reduction,
                'target': self.target_reduction,
                'success': abs(reduction - self.target_reduction) <= 10
            }
            
        except Exception as e:
            print(f"âŒ ANALÄ°Z HATASI: {e}")
            return None


# ============================================================================
# Ã‡ALIÅTIRMA FONKSÄ°YONU
# ============================================================================

def run_smart_stopwords_detection(csv_path, target_reduction=35):
 
    print("\n" + "="*60)
    print("ğŸš€ SMART STOPWORDS DETECTOR - PUTÄ°N KONUÅMALARI")
    print("="*60)
    
    # Parametre kontrolÃ¼
    if not 20 <= target_reduction <= 50:
        return
    
    try:
        # Detector'Ä± baÅŸlat
        detector = SmartStopwordsDetectorOptimized(
            csv_path=csv_path,
            text_column='text',
            target_reduction=target_reduction
        )
        
        # Tam analizi Ã§alÄ±ÅŸtÄ±r
        results = detector.run_full_analysis()
         
    except FileNotFoundError:
        print(f"\nâŒ HATA: '{csv_path}' dosyasÄ± bulunamadÄ±!")
        print("   LÃ¼tfen dosya yolunu kontrol edin.")
    except Exception as e:
        print(f"\nâŒ HATA: {e}")


if __name__ == "__main__":


    CSV_FILE = "speech.csv"
    TARGET_REDUCTION = 35 
    
    run_smart_stopwords_detection(CSV_FILE, TARGET_REDUCTION)

# -*- coding: utf-8 -*-
"""
PUTÄ°N KONUÅMALARI - GELÄ°ÅTÄ°RÄ°LMÄ°Å FÄ°NAL LDA ANALÄ°ZÄ°
Zaman analizi, gÃ¼ven aralÄ±klarÄ± grafikleri ve Ã¶nemli olay iÅŸaretleyicileri eklendi
Konu ayrÄ±ÅŸtÄ±rma iyileÅŸtirildi - STOPWORDS GÃœNCELLENDÄ°
"""

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
import os
from collections import Counter
from datetime import datetime
import random
from matplotlib.dates import DateFormatter
import matplotlib.dates as mdates
from matplotlib.patches import Ellipse
import matplotlib.transforms as transforms

class PutinLDAGelistirilmis:
    """GeliÅŸtirilmiÅŸ Putin LDA analizi"""
    
    def __init__(self, csv_path, random_seed=42, n_topics=None):
        print("="*70)
        print("ğŸ”¥ PUTÄ°N KONUÅMALARI - GELÄ°ÅTÄ°RÄ°LMÄ°Å LDA ANALÄ°ZÄ°")
        print("="*70)
        
        # TÃ¼m random seed'leri sabitle
        self.random_seed = random_seed
        np.random.seed(self.random_seed)
        random.seed(self.random_seed)
        
        self.df = pd.read_csv(csv_path)
        
        if 'date' in self.df.columns:
            self.df['date'] = pd.to_datetime(self.df['date'], errors='coerce')
            self.df = self.df.dropna(subset=['date'])
            self.df['year'] = self.df['date'].dt.year
            self.df['month'] = self.df['date'].dt.month
            self.df['quarter'] = self.df['date'].dt.quarter
            self.df['year_month'] = self.df['date'].dt.to_period('M')
            self.df['year_quarter'] = self.df['date'].dt.to_period('Q')
        
        print(f"âœ“ {len(self.df)} konuÅŸma yÃ¼klendi")
        if 'date' in self.df.columns:
            print(f"âœ“ Tarih aralÄ±ÄŸÄ±: {self.df['date'].min().strftime('%Y-%m')} - {self.df['date'].max().strftime('%Y-%m')}")
        
        self.prepare_texts_gelistirilmis()
        self.n_topics = n_topics
    
    def prepare_texts_gelistirilmis(self):
        """GeliÅŸtirilmiÅŸ metin temizleme - GÃœNCELLENMÄ°Å STOPWORDS"""
        print("\nğŸ§¹ METÄ°NLER GELÄ°ÅTÄ°RÄ°LMÄ°Å TEMÄ°ZLENÄ°YOR...")
        
        if 'filtered_text' in self.df.columns:
            raw_texts = self.df['filtered_text'].fillna('').tolist()
        else:
            raw_texts = self.df['text'].fillna('').tolist()
        
        # GÃœNCELLENMÄ°Å stopwords listesi - TÃœM VERÄ°LEN STOPWORDS EKLENDÄ°
        GUNCELLENMIS_STOPWORDS = set([
            # Pronouns
            'i', 'me', 'my', 'myself', 'we', 'us', 'our', 'ours', 'ourselves',
            'you', 'your', 'yours', 'yourself', 'yourselves',
            'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',
            'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'there', 'these',
            
            # Common verbs
            'be', 'is', 'am', 'are', 'was', 'were', 'been', 'being',
            'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',
            'say', 'says', 'said',
            
            # Quantifiers
            'the', 'a', 'an', 'and', 'or', 'but', 'if', 'because', 'as',
            'what', 'which', 'who', 'whom', 'whose', 'where', 'when', 'how', 'why',
            'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',
            'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',
            'too', 'very', 'can', 'will', 'just', 'should', 'now',
            
            # Ã‡ok genel kelimeler
            'question', 'year', 'then', 'yes', 'no',
            'many', 'much', 'also', 'very', 'really',
            'well', 'good', 'better', 'best',
            
            # Putin'e Ã¶zel
            'putin', 'vladimir', 'president',
            'thank', 'thanks', 'thank you', 'ladies gentlemen',
            'ladies', 'gentlemen',
            
            # Para birimleri ve miktarlar
            'million tonnes', 'billion rubles', 'million dollars', 'billion dollars',
            'trillion rubles', 'thousand tonnes',
            
            # Zaman ifadeleri
            'three years', 'eight years', 'past years', 'recent years',
            'next years', 'coming years', 'previous years',
            
            # Genel miktar ifadeleri
            'large scale', 'small scale', 'high level', 'low level',
            'great deal', 'first time', 'last time',
            
            # KarÅŸÄ±laÅŸtÄ±rmalar
            'compared with', 'compared to', 'in comparison',
            
            # Tek kelime olarak da ekle
            'million', 'billion', 'trillion', 'thousand',
            'tonnes', 'rubles', 'dollars', 'euros',
            'years', 'months', 'weeks', 'days', 'talking', 'about', 'first', 'point',
            'between', 'think', 'about', 'during', 'dmitry', 'peskov', 'pavel', 'zarubin',
            'would', 'like', 'would like', 'russian', 'federation', 'into', 'account', 'comrade',
            'saudi', 'arabia', 'please', 'ahead', 'alexander', 'lukashenko', 'families', 'children',
            'fyodor', 'lukyanov', 'over','past', 'people', 'republic','prime', 'minister',
            'long', 'term','time',
            'afternoon', 'name','lvova', 'belova', 'kherson','konstantin', 'panyushkin',
            'proceed','from','around','world',
            'commander','chief','extremely','important','large','scale','small','medium','sized',
            'among','things','make','sure','minimum','wage',
            'medical', 'check', 'everything','done', 'continue','work','took','part','even' ,'though',
            'arab', 'emirates','percent','business','countries','taking', 'place', 'without', 'doubt','want','emphasise'

        ])
        
        def gelistirilmis_clean_text(text):
            text = str(text).lower()
            
            # Noktalama ve sayÄ±larÄ± kaldÄ±r
            text = re.sub(r'[^\w\s]', ' ', text)
            text = re.sub(r'\d+', '', text)
            
            # Kelimelere ayÄ±r
            words = text.split()
            
            # GeliÅŸtirilmiÅŸ filtreleme
            filtered_words = []
            for w in words:
                if len(w) < 4:  # 4 karakterden kÄ±sa kelimeleri filtrele
                    continue
                if w in GUNCELLENMIS_STOPWORDS:
                    continue
                filtered_words.append(w)
            
            cleaned_text = ' '.join(filtered_words)
            cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
            
            return cleaned_text
        
        self.texts = []
        for i, t in enumerate(raw_texts):
            cleaned = gelistirilmis_clean_text(t)
            self.texts.append(cleaned)
        
        total_words = sum(len(t.split()) for t in self.texts)
        unique_words = len(set(' '.join(self.texts).split()))
        
        print(f"âœ“ Toplam kelime: {total_words:,}")
        print(f"âœ“ Benzersiz kelime: {unique_words:,}")
        print(f"âœ“ Ortalama kelime/konuÅŸma: {total_words/len(self.texts):.0f}")
        
        return self.texts
    
    def create_dtm_gelistirilmis(self, n_topics=5):
        """GeliÅŸtirilmiÅŸ DTM - daha iyi konu ayrÄ±ÅŸmasÄ± iÃ§in"""
        print("\nğŸ“Š GELÄ°ÅTÄ°RÄ°LMÄ°Å DOCUMENT-TERM MATRIX OLUÅTURULUYOR...")
        
        # KONU AYRIÅMASI Ä°Ã‡Ä°N OPTÄ°MÄ°ZE PARAMETRELER
        if n_topics == 5:
            max_features = 1200  # ArtÄ±rÄ±ldÄ± - daha fazla Ã¶zellik
            min_df = 3  # AzaltÄ±ldÄ± - daha fazla n-gram
            max_df = 0.5  # AzaltÄ±ldÄ± - Ã§ok yaygÄ±n terimleri filtrele
            ngram_range = (2, 4)  # GeniÅŸletildi - 4-gram'a kadar
        else:
            max_features = 1000
            min_df = 4
            max_df = 0.6
            ngram_range = (2, 3)
        
        print(f"  â†’ N-gram: {ngram_range[0]}-{ngram_range[1]}")
        print(f"  â†’ Max Ã¶zellik: {max_features}")
        print(f"  â†’ Min DF: {min_df}")
        print(f"  â†’ Max DF: %{max_df*100:.0f}")
        print(f"  â†’ Random seed: {self.random_seed}")
        
        self.vectorizer = CountVectorizer(
            max_features=max_features,
            min_df=min_df,
            max_df=max_df,
            stop_words=None,  # Stopwords'leri kendimiz filtreledik
            ngram_range=ngram_range,
            lowercase=True,
            token_pattern=r'\b[a-zA-Z]{4,}\b',  # 4 karakterden azlarÄ± filtrele
        )
        
        self.dtm = self.vectorizer.fit_transform(self.texts)
        
        print(f"âœ“ Belge sayÄ±sÄ±: {self.dtm.shape[0]}")
        print(f"âœ“ Terim sayÄ±sÄ±: {self.dtm.shape[1]}")
        
        # En sÄ±k terimleri gÃ¶ster
        feature_names = self.vectorizer.get_feature_names_out()
        term_freq = np.asarray(self.dtm.sum(axis=0)).flatten()
        top_indices = term_freq.argsort()[-15:][::-1]
        
        print(f"\nğŸ” EN SIK 15 N-GRAM:")
        print("-" * 50)
        for idx in top_indices[:15]:
            term = feature_names[idx]
            freq = term_freq[idx]
            percentage = (freq / self.dtm.sum()) * 100
            print(f"  {term:40} â†’ {freq:6,} kez (%{percentage:.2f})")
        
        return self.dtm
    
    def perform_lda_gelistirilmis(self, n_topics=5):
        """GeliÅŸtirilmiÅŸ LDA - daha iyi konu ayrÄ±ÅŸmasÄ±"""
        print(f"\n" + "="*70)
        print(f"ğŸ§  GELÄ°ÅTÄ°RÄ°LMÄ°Å LDA ANALÄ°ZÄ° ({n_topics} KONU)")
        print("="*70)
        
        # GELÄ°ÅTÄ°RÄ°LMÄ°Å LDA PARAMETRELERÄ° - daha iyi ayrÄ±ÅŸma iÃ§in
        print(f"\nğŸ¯ GELÄ°ÅTÄ°RÄ°LMÄ°Å LDA PARAMETRELERÄ°:")
        print(f"  â€¢ Konu sayÄ±sÄ±: {n_topics}")
        print(f"  â€¢ Random seed: {self.random_seed}")
        print(f"  â€¢ Max iterasyon: 50")  # ArtÄ±rÄ±ldÄ±
        print(f"  â€¢ Doc-topic prior: 0.1")  # AzaltÄ±ldÄ± - daha sÄ±kÄ± daÄŸÄ±lÄ±m
        print(f"  â€¢ Topic-word prior: 0.01")  # AzaltÄ±ldÄ± - daha spesifik kelimeler
        print(f"  â€¢ Learning decay: 0.7")  # Daha yavaÅŸ Ã¶ÄŸrenme
        
        self.lda = LatentDirichletAllocation(
            n_components=n_topics,
            random_state=self.random_seed,
            learning_method='online',
            max_iter=50,  # ArtÄ±rÄ±ldÄ±
            learning_offset=10.0,
            learning_decay=0.7,  # Optimize edildi
            doc_topic_prior=0.1,  # AzaltÄ±ldÄ±
            topic_word_prior=0.01,  # AzaltÄ±ldÄ±
            n_jobs=-1,
            verbose=1
        )
        
        print("\nğŸ“š LDA modeli eÄŸitiliyor...")
        self.lda.fit(self.dtm)
        
        print(f"\nâœ“ Model eÄŸitimi tamamlandÄ±")
        print(f"âœ“ Final perplexity: {self.lda.perplexity(self.dtm):.1f}")
        
        # KonularÄ± Ã§Ä±kar
        feature_names = self.vectorizer.get_feature_names_out()
        
        print(f"\nğŸ¯ {n_topics} KONU BULUNDU:")
        print("="*70)
        
        self.topics = []
        
        for topic_idx, topic in enumerate(self.lda.components_):
            # Her konu iÃ§in top 15 n-gram (artÄ±rÄ±ldÄ±)
            top_indices = topic.argsort()[-15:][::-1]
            top_ngrams = [feature_names[i] for i in top_indices]
            top_weights = [topic[i] for i in top_indices]
            
            # GELÄ°ÅTÄ°RÄ°LMÄ°Å konu etiketleme
            topic_label = self.interpret_topic_gelistirilmis(top_ngrams, topic_idx, n_topics)
            
            self.topics.append({
                'id': topic_idx,
                'label': topic_label,
                'keywords': top_ngrams,
                'weights': top_weights,
                'top_keywords': top_ngrams[:6]
            })
            
            print(f"\nğŸ“Œ KONU {topic_idx + 1}: {topic_label}")
            print("-" * 50)
            
            print("  ğŸ”‘ Ã–NEMLÄ° N-GRAM'LAR:")
            for i in range(0, min(15, len(top_ngrams)), 5):
                chunk = top_ngrams[i:i+5]
                if chunk:
                    print(f"     â€¢ {', '.join(chunk)}")
        
        # DokÃ¼man-konu daÄŸÄ±lÄ±mÄ±
        self.topic_distribution = self.lda.transform(self.dtm)
        self.df['dominant_topic'] = self.topic_distribution.argmax(axis=1)
        self.df['topic_confidence'] = self.topic_distribution.max(axis=1)
        
        # Konu gÃ¼ven aralÄ±klarÄ±nÄ± hesapla
        self.calculate_topic_confidence_intervals()
        
        return self.lda
    
    def calculate_topic_confidence_intervals(self):
        """Konu bazÄ±nda gÃ¼ven aralÄ±klarÄ±nÄ± hesapla"""
        print("\nğŸ“Š KONU BAZINDA GÃœVEN ARALIKLARI HESAPLANIYOR...")
        
        self.topic_confidence_stats = []
        
        for topic in self.topics:
            topic_id = topic['id']
            topic_confidences = self.df[self.df['dominant_topic'] == topic_id]['topic_confidence']
            
            if len(topic_confidences) > 0:
                mean_confidence = topic_confidences.mean()
                std_confidence = topic_confidences.std()
                n_samples = len(topic_confidences)
                
                # %95 gÃ¼ven aralÄ±ÄŸÄ±
                if n_samples > 1:
                    # t-daÄŸÄ±lÄ±mÄ± kullan
                    t_value = stats.t.ppf(0.975, n_samples - 1)
                    margin_of_error = t_value * (std_confidence / np.sqrt(n_samples))
                    ci_lower = mean_confidence - margin_of_error
                    ci_upper = mean_confidence + margin_of_error
                else:
                    ci_lower = mean_confidence
                    ci_upper = mean_confidence
                
                # Konfidans seviyesi kategorisi
                if mean_confidence >= 0.8:
                    confidence_level = "Ã‡OK YÃœKSEK"
                    level_color = "ğŸŸ¢"
                elif mean_confidence >= 0.6:
                    confidence_level = "YÃœKSEK"
                    level_color = "ğŸŸ¡"
                elif mean_confidence >= 0.4:
                    confidence_level = "ORTA"
                    level_color = "ğŸŸ "
                else:
                    confidence_level = "DÃœÅÃœK"
                    level_color = "ğŸ”´"
                
                self.topic_confidence_stats.append({
                    'topic_id': topic_id,
                    'topic_label': topic['label'],
                    'n_documents': n_samples,
                    'mean_confidence': mean_confidence,
                    'std_confidence': std_confidence,
                    'ci_lower': ci_lower,
                    'ci_upper': ci_upper,
                    'margin_of_error': margin_of_error if n_samples > 1 else 0,
                    'confidence_level': confidence_level,
                    'level_color': level_color
                })
                
                print(f"  â€¢ Konu {topic_id+1}: {topic['label'][:40]}...")
                print(f"    â†’ Ortalama gÃ¼ven: {mean_confidence:.3f} Â± {margin_of_error:.3f}")
                print(f"    â†’ %95 GA: [{ci_lower:.3f}, {ci_upper:.3f}]")
                print(f"    â†’ Seviye: {confidence_level} {level_color}")
        
        return self.topic_confidence_stats
    
    def interpret_topic_gelistirilmis(self, ngrams, topic_id, n_topics):
        """GeliÅŸtirilmiÅŸ konu etiketleme - BENZERSÄ°Z etiketler iÃ§in"""
        
        # TÃ¼m n-gram'larÄ± analiz et
        ngrams_text = ' '.join(ngrams).lower()
        
        # Ã‡OK SPESÄ°FÄ°K KONTROLLER - benzersiz etiketler iÃ§in
        if 'kiev regime' in ngrams_text and 'donetsk' in ngrams_text:
            return 'UKRAYNA: KIEV REJÄ°MÄ° VE DONBAS BAÄIMSIZLIÄI'
        
        elif 'terrorist attack' in ngrams_text and 'crimean bridge' in ngrams_text:
            return 'TERÃ–R SALDIRILARI: KRÄ°M KÃ–PRÃœSÃœ GÃœVENLÄ°ÄÄ°'
        
        elif 'siege leningrad' in ngrams_text:
            return 'TARÄ°HSEL ANILAR: LENÄ°NGRAD KUÅATMASI'
        
        elif 'great patriotic war' in ngrams_text:
            return 'TARÄ°HSEL MÄ°RAS: BÃœYÃœK VATANSEVERLÄ°K SAVAÅI'
        
        elif 'nuclear weapons' in ngrams_text:
            return 'STRATEJÄ°K SAVUNMA: NÃœKLEER SÄ°LAH SÄ°STEMLERÄ°'
        
        elif 'artificial intelligence' in ngrams_text:
            return 'TEKNOLOJÄ°K DEVRÄ°M: YAPAY ZEKA GELÄ°ÅÄ°MÄ°'
        
        elif 'middle east' in ngrams_text:
            return 'DIÅ POLÄ°TÄ°KA: ORTA DOÄU DÄ°PLOMASÄ°SÄ°'
        
        elif 'south africa' in ngrams_text:
            return 'ULUSLARARASI Ä°LÄ°ÅKÄ°LER: AFRÄ°KA Ä°ÅBÄ°RLÄ°ÄÄ°'
        
        elif 'energy resources' in ngrams_text:
            return 'EKONOMÄ°K POLÄ°TÄ°KA: ENERJÄ° KAYNAKLARI'
        
        elif 'economic sanctions' in ngrams_text:
            return 'EKONOMÄ°K MÃœCADELE: YAPTIRIMLAR VE FÄ°NANS'
        
        elif 'special military operation' in ngrams_text:
            return 'ASKERÄ° STRATEJÄ°: Ã–ZEL ASKERÄ° OPERASYON'
        
        # Kelime frekanslarÄ±na gÃ¶re benzersiz etiket
        word_counter = Counter()
        for ngram in ngrams[:10]:
            words = ngram.split()
            for word in words:
                word = word.lower()
                if len(word) > 5:  # Uzun kelimeleri tercih et
                    word_counter[word] += 1
        
        # En sÄ±k 3 benzersiz kelime
        top_words = []
        seen_words = set()
        for word, count in word_counter.most_common(10):
            if word not in seen_words and len(top_words) < 3:
                top_words.append(word.upper())
                seen_words.add(word)
        
        # Konu ID'sine gÃ¶re Ã¶zel etiketler (n_topics'e gÃ¶re)
        if n_topics == 5:
            specific_labels = {
                0: 'UKRAYNA SAVAÅI: ASKERÄ° OPERASYONLAR VE STRATEJÄ°',
                1: 'TARÄ°HSEL HAFIZA: SAVAÅ ANILARI VE MÄ°LLÄ° KÄ°MLÄ°K',
                2: 'DEVLET YÃ–NETÄ°MÄ°: Ä°Ã‡ POLÄ°TÄ°KA VE KURUMSAL REFORMLAR',
                3: 'EKONOMÄ°K KALKINMA: TEKNOLOJÄ° VE SANAYÄ° POLÄ°TÄ°KASI',
                4: 'ULUSLARARASI DÄ°PLOMASÄ°: BÃ–LGESEL Ä°ÅBÄ°RLÄ°KLERÄ°'
            }
            if topic_id in specific_labels:
                return specific_labels[topic_id]
        
        # Benzersiz etiket oluÅŸtur
        if len(top_words) >= 2:
            return f"{top_words[0]} VE {top_words[1]} POLÄ°TÄ°KALARI"
        
        return f"KONU {topic_id+1}: SÄ°YASÄ° ANALÄ°Z"
    
    def zaman_analizi_grafikleri(self):
        """Zaman analizi grafikleri - ALT ALTA DÃœZEN ve Ã–NEMLÄ° OLAYLAR EKLENDÄ°"""
        print("\nâ° ZAMAN ANALÄ°ZÄ° GRAFÄ°KLERÄ° OLUÅTURULUYOR (Ã–NEMLÄ° OLAYLAR EKLENDÄ°)...")
        
        if 'date' not in self.df.columns:
            print("âš ï¸  Tarih verisi yok, zaman analizi yapÄ±lamÄ±yor")
            return
        
        # AylÄ±k konu daÄŸÄ±lÄ±mÄ±
        self.df['year_month_dt'] = self.df['date'].dt.to_period('M').dt.to_timestamp()
        
        # YarÄ±yÄ±l (yarÄ±m yÄ±l) hesapla
        self.df['half_year'] = self.df['date'].dt.year.astype(str) + '-' + self.df['date'].dt.quarter.apply(
            lambda q: 'H1' if q <= 2 else 'H2'
        )
        
        # Konu etiketlerini DataFrame'e ekle
        topic_labels = {t['id']: t['label'] for t in self.topics}
        self.df['topic_label'] = self.df['dominant_topic'].map(topic_labels)
        
        # Zaman serisi analizi
        monthly_data = self.df.groupby(['year_month_dt', 'topic_label']).size().unstack(fill_value=0)
        
        # GRAFÄ°K 1: Zaman iÃ§inde konu daÄŸÄ±lÄ±mÄ± (alan grafiÄŸi) - TEK BAÅINA
        fig1, ax1 = plt.subplots(figsize=(14, 6))
        colors = plt.cm.Set3(np.linspace(0, 1, len(monthly_data.columns)))
        
        monthly_data.plot.area(ax=ax1, alpha=0.8, color=colors)
        
        # Ã–NEMLÄ° OLAYLARI EKLE - Dikey Ã§izgiler
        olaylar = [
            {'tarih': '2022-02-24', 'etiket': 'Ukrayna Ä°ÅŸgali', 'renk': 'red', 'alpha': 0.7},
            {'tarih': '2022-03-12', 'etiket': 'SWIFT YaptÄ±rÄ±mlarÄ±', 'renk': 'orange', 'alpha': 0.7},
            {'tarih': '2022-10-08', 'etiket': 'KÄ±rÄ±m KÃ¶prÃ¼sÃ¼ PatlamasÄ±', 'renk': 'darkred', 'alpha': 0.7},
            {'tarih': '2022-11-11', 'etiket': 'Herson Geri AlÄ±ndÄ±', 'renk': 'green', 'alpha': 0.7},
            {'tarih': '2023-06-23', 'etiket': 'Wagner Ä°syanÄ±', 'renk': 'purple', 'alpha': 0.7},
            {'tarih': '2023-04-04', 'etiket': 'Finlandiya NATO', 'renk': 'blue', 'alpha': 0.7},
            {'tarih': '2025-02-28', 'etiket': 'Trump-Zelenski Krizi', 'renk': 'brown', 'alpha': 0.7},
            {'tarih': '2025-08-15', 'etiket': 'Trump-Putin Alaska', 'renk': 'cyan', 'alpha': 0.7},
            {'tarih': '2025-11-21', 'etiket': 'Trump BarÄ±ÅŸ PlanÄ±', 'renk': 'magenta', 'alpha': 0.7}
        ]
        
        for olay in olaylar:
            olay_tarih = pd.to_datetime(olay['tarih'])
            if ax1.get_xlim()[0] <= mdates.date2num(olay_tarih) <= ax1.get_xlim()[1]:
                ax1.axvline(x=olay_tarih, color=olay['renk'], linestyle='--', 
                          alpha=olay['alpha'], linewidth=2)
                # Etiketi ekle
                ax1.text(olay_tarih, ax1.get_ylim()[1]*0.95, olay['etiket'], 
                       rotation=90, verticalalignment='top',
                       color=olay['renk'], fontsize=8, fontweight='bold',
                       alpha=0.8)
        
        ax1.set_title(f'Putin KonuÅŸmalarÄ± - Zaman Ä°Ã§inde Konu DaÄŸÄ±lÄ±mÄ± (Seed: {self.random_seed})', 
                     fontsize=12, fontweight='bold')
        ax1.set_xlabel('Tarih')
        ax1.set_ylabel('KonuÅŸma SayÄ±sÄ±')
        ax1.legend(title='Konular', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        ax1.grid(True, alpha=0.3)
        
        # Tarih formatÄ±
        ax1.xaxis.set_major_formatter(DateFormatter('%Y-%m'))
        ax1.xaxis.set_major_locator(mdates.MonthLocator(interval=3))
        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)
        plt.tight_layout()
        plt.show()
        
        # GRAFÄ°K 2: YARIM YILLIK KONU DAÄILIMI - TEK BAÅINA
        fig2, ax2 = plt.subplots(figsize=(14, 6))
        
        # YarÄ±yÄ±l (yarÄ±m yÄ±l) bazÄ±nda konu daÄŸÄ±lÄ±mÄ±
        half_year_data = self.df.groupby(['half_year', 'topic_label']).size().unstack(fill_value=0)
        
        # YarÄ±yÄ±llarÄ± sÄ±rala
        half_year_data = half_year_data.sort_index()
        
        # Son 8 yarÄ±yÄ±l
        last_8_half_years = half_year_data.index[-8:] if len(half_year_data) > 8 else half_year_data.index
        
        half_year_data.loc[last_8_half_years].plot(kind='bar', stacked=True, ax=ax2, 
                                                   alpha=0.85, color=colors)
        
        # Ã–nemli olaylarÄ±n olduÄŸu yarÄ±yÄ±llarÄ± vurgula
        olay_cizelgesi = {
            '2022-H1': 'Ukrayna Ä°ÅŸgali',
            '2022-H1': 'SWIFT YaptÄ±rÄ±mlarÄ±',
            '2022-H2': 'KÄ±rÄ±m KÃ¶prÃ¼sÃ¼',
            '2022-H2': 'Herson KurtarÄ±ldÄ±',
            '2023-H1': 'Finlandiya NATO',
            '2023-H2': 'Wagner Ä°syanÄ±',
            '2025-H1': 'Trump-Zelenski',
            '2025-H2': 'Alaska Zirvesi',
            '2025-H2': 'Trump BarÄ±ÅŸ PlanÄ±'
        }
        
        # X ekseni etiketlerini vurgula
        xticklabels = ax2.get_xticklabels()
        for i, label in enumerate(xticklabels):
            half_year_str = label.get_text()
            if half_year_str in olay_cizelgesi:
                label.set_color('red')
                label.set_fontweight('bold')
        
        ax2.set_title('YarÄ±m YÄ±llÄ±k Konu DaÄŸÄ±lÄ±mÄ± - Ã–nemli Olaylar Ä°ÅŸaretlendi', fontsize=12, fontweight='bold')
        ax2.set_xlabel('YarÄ±m YÄ±l (H1: Ocak-Haziran, H2: Temmuz-AralÄ±k)')
        ax2.set_ylabel('KonuÅŸma SayÄ±sÄ±')
        ax2.legend(title='Konular', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        ax2.grid(True, alpha=0.3, axis='y')
        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
        plt.tight_layout()
        plt.show()
        
        # KONU BAZINDA YARIM YILLIK DAÄILIM TABLOSU
        print("\nğŸ“‹ KONU BAZINDA YARIM YILLIK DAÄILIM TABLOSU:")
        print("="*80)
        
        # YarÄ±m yÄ±llÄ±k konu daÄŸÄ±lÄ±mÄ±nÄ± tablo olarak hazÄ±rla
        half_year_table = self.df.groupby(['half_year', 'topic_label']).size().unstack(fill_value=0)
        
        # YarÄ±yÄ±llarÄ± sÄ±rala
        half_year_table = half_year_table.sort_index()
        
        # Toplam satÄ±rÄ± ekle
        half_year_table.loc['TOPLAM'] = half_year_table.sum()
        
        # YÃ¼zdelik daÄŸÄ±lÄ±mÄ± hesapla
        half_year_percentage = half_year_table.div(half_year_table.sum(axis=1), axis=0) * 100
        
        # Tabloyu gÃ¶ster
        print(f"\n{'YarÄ±m YÄ±l':<12} | ", end="")
        for topic_label in half_year_table.columns:
            short_label = topic_label[:20] + ('...' if len(topic_label) > 20 else '')
            print(f"{short_label:<20} | ", end="")
        print("Toplam")
        print("-" * (12 + len(half_year_table.columns) * 24))
        
        for half_year in half_year_table.index:
            if half_year == 'TOPLAM':
                print("\n" + "=" * (12 + len(half_year_table.columns) * 24))
            
            print(f"{half_year:<12} | ", end="")
            total = 0
            for topic_label in half_year_table.columns:
                count = half_year_table.loc[half_year, topic_label]
                percentage = half_year_percentage.loc[half_year, topic_label]
                total += count
                
                if count > 0:
                    print(f"{count:3d} (%{percentage:5.1f}){' ':<10}", end="")
                else:
                    print(f"{' - ':<20}", end="")
            print(f"| {total:4d}")
        
        # Konu bazÄ±nda Ã¶zet tablo
        print(f"\n\nğŸ“Š KONU BAZINDA Ã–ZET DAÄILIM:")
        print("="*80)
        
        topic_summary = []
        for topic in self.topics:
            topic_id = topic['id']
            topic_label = topic['label']
            
            # Bu konunun yarÄ±yÄ±llÄ±k daÄŸÄ±lÄ±mÄ±
            topic_half_year_dist = self.df[self.df['dominant_topic'] == topic_id].groupby('half_year').size()
            
            # Toplam belge sayÄ±sÄ±
            total_docs = topic_half_year_dist.sum()
            
            # En aktif yarÄ±yÄ±l
            if len(topic_half_year_dist) > 0:
                most_active_half_year = topic_half_year_dist.idxmax()
                most_active_count = topic_half_year_dist.max()
                most_active_percentage = (most_active_count / total_docs * 100) if total_docs > 0 else 0
            else:
                most_active_half_year = '-'
                most_active_count = 0
                most_active_percentage = 0
            
            topic_summary.append({
                'Konu': f"K{topic_id+1}",
                'Konu AdÄ±': topic_label[:50] + ('...' if len(topic_label) > 50 else ''),
                'Toplam': total_docs,
                'En Aktif YarÄ±yÄ±l': most_active_half_year,
                'En Aktif SayÄ±': most_active_count,
                'En Aktif %': f"%{most_active_percentage:.1f}"
            })
        
        # Tabloyu gÃ¶ster
        summary_df = pd.DataFrame(topic_summary)
        print("\n" + summary_df.to_string(index=False))
        
        # GRAFÄ°K 3: Ã–nemli olaylar sonrasÄ± konu yoÄŸunluÄŸu - TEK BAÅINA
        fig3, ax3 = plt.subplots(figsize=(14, 6))
        
        # Ã–nemli olaylar sonrasÄ± 30 gÃ¼nlÃ¼k periyotlarÄ± analiz et
        olay_periyotlari = []
        olay_etiketler = []
        
        for olay in olaylar[:6]:  # Ä°lk 6 Ã¶nemli olay
            olay_tarih = pd.to_datetime(olay['tarih'])
            baslangic = olay_tarih - pd.Timedelta(days=15)
            bitis = olay_tarih + pd.Timedelta(days=15)
            
            # Bu periyottaki konuÅŸmalarÄ± filtrele
            periyot_konusmalar = self.df[(self.df['date'] >= baslangic) & (self.df['date'] <= bitis)]
            
            if len(periyot_konusmalar) > 0:
                konu_dagilimi = periyot_konusmalar['dominant_topic'].value_counts(normalize=True)
                
                # Her konu iÃ§in yÃ¼zdeyi al
                for konu_id in range(len(self.topics)):
                    yuzde = konu_dagilimi.get(konu_id, 0) * 100
                    olay_periyotlari.append({
                        'olay': olay['etiket'],
                        'konu': self.topics[konu_id]['label'][:30],
                        'yuzde': yuzde
                    })
        
        if olay_periyotlari:
            olay_df = pd.DataFrame(olay_periyotlari)
            pivot_df = olay_df.pivot_table(index='olay', columns='konu', values='yuzde', aggfunc='mean')
            
            # Grafik
            pivot_df.plot(kind='bar', ax=ax3, alpha=0.8, figsize=(14, 6))
            ax3.set_title('Ã–nemli Olaylar SonrasÄ± Konu DaÄŸÄ±lÄ±mÄ± (30 GÃ¼nlÃ¼k Periyot)', 
                         fontsize=12, fontweight='bold')
            ax3.set_xlabel('Olay')
            ax3.set_ylabel('Konu YÃ¼zdesi (%)')
            ax3.legend(title='Konular', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
            ax3.grid(True, alpha=0.3, axis='y')
            plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
            plt.tight_layout()
            plt.show()
        
        # GRAFÄ°K 4: Zaman Ã§izelgesi - olaylarÄ±n kronolojik gÃ¶sterimi
        fig4, ax4 = plt.subplots(figsize=(14, 6))
        
        # Basit zaman Ã§izelgesi
        ax4.set_xlim(pd.to_datetime('2022-01-01'), pd.to_datetime('2025-12-31'))
        ax4.set_ylim(0, len(olaylar) + 1)
        
        # OlaylarÄ± ekle
        for i, olay in enumerate(olaylar):
            olay_tarih = pd.to_datetime(olay['tarih'])
            ax4.plot([olay_tarih, olay_tarih], [0, i+1], '--', color=olay['renk'], alpha=0.5)
            ax4.scatter(olay_tarih, i+1, color=olay['renk'], s=100, alpha=0.8)
            ax4.text(olay_tarih + pd.Timedelta(days=30), i+1, olay['etiket'], 
                   verticalalignment='center', fontsize=9, fontweight='bold',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.7))
        
        ax4.set_yticks(range(1, len(olaylar)+1))
        ax4.set_yticklabels([f"{i+1}. {olaylar[i]['etiket']}" for i in range(len(olaylar))])
        ax4.set_title('Ã–nemli OlaylarÄ±n Kronolojik Zaman Ã‡izelgesi', fontsize=12, fontweight='bold')
        ax4.set_xlabel('Tarih')
        ax4.grid(True, alpha=0.3)
        ax4.xaxis.set_major_formatter(DateFormatter('%Y-%m'))
        ax4.xaxis.set_major_locator(mdates.MonthLocator(interval=4))
        plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)
        plt.tight_layout()
        plt.show()
        
        # Ek zaman analizi istatistikleri
        print("\nğŸ“Š ZAMAN ANALÄ°ZÄ° Ä°STATÄ°STÄ°KLERÄ°:")
        print("-" * 50)
        
        # En aktif ay
        monthly_counts = self.df.groupby('year_month_dt').size()
        if len(monthly_counts) > 0:
            most_active_month = monthly_counts.idxmax()
            most_active_count = monthly_counts.max()
            print(f"â€¢ En aktif ay: {most_active_month.strftime('%Y-%m')} ({most_active_count} konuÅŸma)")
        
        # En aktif yarÄ±yÄ±l
        half_year_counts = self.df.groupby('half_year').size()
        if len(half_year_counts) > 0:
            most_active_half_year = half_year_counts.idxmax()
            most_active_half_year_count = half_year_counts.max()
            print(f"â€¢ En aktif yarÄ±yÄ±l: {most_active_half_year} ({most_active_half_year_count} konuÅŸma)")
        
        # Konu trendleri (yarÄ±m yÄ±llÄ±k)
        print("\nğŸ“ˆ YARIM YILLIK KONU TRENDLERÄ°:")
        if len(half_year_data) >= 4:
            last_4_half_years = half_year_data.index[-4:]
            for topic_label in half_year_data.columns:
                trend_data = half_year_data.loc[last_4_half_years, topic_label]
                if trend_data.sum() > 0:
                    # Trend analizi
                    if len(trend_data) >= 2:
                        first_half = trend_data.iloc[:2].mean()
                        second_half = trend_data.iloc[2:].mean()
                        if second_half > first_half * 1.3:
                            trend = "ğŸ“ˆ GÃœÃ‡LÃœ YÃœKSELÄ°Å"
                        elif second_half > first_half * 1.1:
                            trend = "ğŸ“ˆ YÃœKSELEN"
                        elif second_half < first_half * 0.7:
                            trend = "ğŸ“‰ KESKÄ°N DÃœÅÃœÅ"
                        elif second_half < first_half * 0.9:
                            trend = "ğŸ“‰ DÃœÅEN"
                        else:
                            trend = "â¡ï¸  STABÄ°L"
                        
                        short_label = topic_label[:35] + ('...' if len(topic_label) > 35 else '')
                        print(f"  {short_label:40} â†’ {trend}")
        
        # Ã–nemli olaylarÄ±n etkisi
        print(f"\nğŸ¯ Ã–NEMLÄ° OLAYLARIN ANALÄ°ZÄ°:")
        print("-" * 50)
        for olay in olaylar[:3]:  # Ä°lk 3 Ã¶nemli olayÄ± analiz et
            olay_tarih = pd.to_datetime(olay['tarih'])
            # Ã–nceki 30 gÃ¼n
            onceki_periyot = self.df[(self.df['date'] >= olay_tarih - pd.Timedelta(days=30)) & 
                                   (self.df['date'] < olay_tarih)]
            # Sonraki 30 gÃ¼n
            sonraki_periyot = self.df[(self.df['date'] > olay_tarih) & 
                                    (self.df['date'] <= olay_tarih + pd.Timedelta(days=30))]
            
            if len(onceki_periyot) > 0 and len(sonraki_periyot) > 0:
                print(f"\nğŸ“… {olay['etiket']} ({olay_tarih.strftime('%d.%m.%Y')}):")
                print(f"   â€¢ Ã–nceki 30 gÃ¼n: {len(onceki_periyot)} konuÅŸma")
                print(f"   â€¢ Sonraki 30 gÃ¼n: {len(sonraki_periyot)} konuÅŸma")
                print(f"   â€¢ DeÄŸiÅŸim: {((len(sonraki_periyot)-len(onceki_periyot))/len(onceki_periyot)*100):+.1f}%")
                
                # En Ã§ok deÄŸiÅŸen konu
                onceki_konular = onceki_periyot['dominant_topic'].value_counts(normalize=True)
                sonraki_konular = sonraki_periyot['dominant_topic'].value_counts(normalize=True)
                
                for konu_id in range(len(self.topics)):
                    onceki_yuzde = onceki_konular.get(konu_id, 0) * 100
                    sonraki_yuzde = sonraki_konular.get(konu_id, 0) * 100
                    if abs(sonraki_yuzde - onceki_yuzde) > 10:  # %10'dan fazla deÄŸiÅŸim
                        konu_adi = self.topics[konu_id]['label'][:30]
                        print(f"   â€¢ {konu_adi}: {onceki_yuzde:.1f}% â†’ {sonraki_yuzde:.1f}% "
                              f"({sonraki_yuzde-onceki_yuzde:+.1f}%)")
        
        return monthly_data
    
    def konu_bazinda_guven_grafikleri(self):
        """KONU BAZINDA DAÄILIM iÃ§in gÃ¼ven aralÄ±klarÄ± grafikleri"""
        print("\nğŸ“Š KONU BAZINDA GÃœVEN ARALIKLARI GRAFÄ°KLERÄ° OLUÅTURULUYOR...")
        
        if not hasattr(self, 'topic_confidence_stats') or not self.topic_confidence_stats:
            print("âš ï¸  GÃ¼ven aralÄ±klarÄ± hesaplanmamÄ±ÅŸ, Ã¶nce LDA analizi yapÄ±n")
            return
        
        # GRAFÄ°K 1: Konu bazÄ±nda gÃ¼ven aralÄ±klarÄ± (error bar) - TEK BAÅINA
        fig1, ax1 = plt.subplots(figsize=(14, 6))
        
        stats_df = pd.DataFrame(self.topic_confidence_stats)
        
        # Konu etiketlerini kÄ±salt
        short_labels = []
        for label in stats_df['topic_label']:
            if len(label) > 40:
                short_labels.append(f"K{int(stats_df.loc[stats_df['topic_label']==label, 'topic_id'].iloc[0])+1}: {label[:37]}...")
            else:
                short_labels.append(f"K{int(stats_df.loc[stats_df['topic_label']==label, 'topic_id'].iloc[0])+1}: {label}")
        
        # Renkleri gÃ¼ven seviyesine gÃ¶re belirle
        colors = []
        for level in stats_df['confidence_level']:
            if level == "Ã‡OK YÃœKSEK":
                colors.append('green')
            elif level == "YÃœKSEK":
                colors.append('limegreen')
            elif level == "ORTA":
                colors.append('orange')
            else:
                colors.append('red')
        
        # X pozisyonlarÄ±
        x_pos = np.arange(len(stats_df))
        
        # Bar grafiÄŸi
        bars = ax1.bar(x_pos, stats_df['mean_confidence'], 
                      yerr=stats_df['margin_of_error'],
                      capsize=10, alpha=0.7, color=colors,
                      edgecolor='black', linewidth=1.5)
        
        # GÃ¼ven aralÄ±klarÄ±nÄ± Ã§iz
        for i, (_, row) in enumerate(stats_df.iterrows()):
            ax1.plot([i, i], [row['ci_lower'], row['ci_upper']], 
                    color='black', linewidth=2, alpha=0.7)
            # Ortalama noktasÄ±
            ax1.scatter(i, row['mean_confidence'], color='white', 
                       s=100, zorder=5, edgecolor='black', linewidth=1.5)
            # GÃ¼ven aralÄ±ÄŸÄ± deÄŸerleri
            ax1.text(i, row['ci_upper'] + 0.01, f"{row['ci_upper']:.3f}", 
                    ha='center', fontsize=8, fontweight='bold')
            ax1.text(i, row['ci_lower'] - 0.015, f"{row['ci_lower']:.3f}", 
                    ha='center', fontsize=8, fontweight='bold')
        
        ax1.set_xlabel('Konular')
        ax1.set_ylabel('GÃ¼ven Skoru (Ortalama Â± %95 GA)')
        ax1.set_title(f'Konu BazÄ±nda GÃ¼ven AralÄ±klarÄ± - %95 GÃ¼ven Seviyesi (Seed: {self.random_seed})', 
                     fontsize=12, fontweight='bold')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(short_labels, rotation=45, ha='right', fontsize=9)
        ax1.set_ylim(0, 1.0)
        ax1.grid(True, alpha=0.3, axis='y')
        
        # Renk aÃ§Ä±klamasÄ±
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='green', alpha=0.7, label='Ã‡OK YÃœKSEK (â‰¥0.8)'),
            Patch(facecolor='limegreen', alpha=0.7, label='YÃœKSEK (0.6-0.8)'),
            Patch(facecolor='orange', alpha=0.7, label='ORTA (0.4-0.6)'),
            Patch(facecolor='red', alpha=0.7, label='DÃœÅÃœK (<0.4)')
        ]
        ax1.legend(handles=legend_elements, loc='upper right', fontsize=9)
        
        plt.tight_layout()
        plt.show()
        
        # GRAFÄ°K 2: Konu daÄŸÄ±lÄ±mÄ± ve gÃ¼ven iliÅŸkisi (scatter plot) - TEK BAÅINA
        fig2, ax2 = plt.subplots(figsize=(14, 6))
        
        # Bubble chart: x=belge sayÄ±sÄ±, y=ortalama gÃ¼ven, boyut=belge sayÄ±sÄ±, renk=gÃ¼ven seviyesi
        sizes = stats_df['n_documents'] / stats_df['n_documents'].max() * 1000
        
        scatter = ax2.scatter(stats_df['n_documents'], stats_df['mean_confidence'],
                            s=sizes, c=range(len(stats_df)), 
                            cmap='viridis', alpha=0.7, edgecolors='black', linewidth=1)
        
        # Her konu iÃ§in etiket
        for i, (_, row) in enumerate(stats_df.iterrows()):
            ax2.annotate(f"K{int(row['topic_id'])+1}", 
                        (row['n_documents'], row['mean_confidence']),
                        xytext=(5, 5), textcoords='offset points',
                        fontsize=9, fontweight='bold',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
            
            # GÃ¼ven aralÄ±ÄŸÄ± Ã§izgileri
            ax2.plot([row['n_documents'], row['n_documents']], 
                    [row['ci_lower'], row['ci_upper']], 
                    color='gray', alpha=0.5, linewidth=1)
        
        ax2.set_xlabel('Belge SayÄ±sÄ± (Konu PopÃ¼laritesi)')
        ax2.set_ylabel('Ortalama GÃ¼ven Skoru')
        ax2.set_title('Konu PopÃ¼laritesi vs. GÃ¼ven Ä°liÅŸkisi (Bubble Chart)', 
                     fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        # Renk bar'Ä±nÄ± ekle
        cbar = plt.colorbar(scatter, ax=ax2)
        cbar.set_label('Konu SÄ±rasÄ±', rotation=270, labelpad=15)
        
        # Trend Ã§izgisi
        if len(stats_df) > 1:
            z = np.polyfit(stats_df['n_documents'], stats_df['mean_confidence'], 1)
            p = np.poly1d(z)
            ax2.plot(stats_df['n_documents'], p(stats_df['n_documents']), 
                    "r--", alpha=0.5, label='Trend Ã‡izgisi')
            ax2.legend(loc='best')
        
        plt.tight_layout()
        plt.show()
        
        # GRAFÄ°K 3: Konu gÃ¼ven daÄŸÄ±lÄ±mÄ± (violin plot) - TEK BAÅINA
        fig3, ax3 = plt.subplots(figsize=(14, 6))
        
        # Her konu iÃ§in gÃ¼ven skorlarÄ±nÄ± topla
        confidence_data = []
        konu_labels_violin = []
        
        for topic in self.topics:
            topic_id = topic['id']
            topic_confidences = self.df[self.df['dominant_topic'] == topic_id]['topic_confidence'].values
            
            if len(topic_confidences) > 0:
                confidence_data.append(topic_confidences)
                konu_labels_violin.append(f"K{topic_id+1}")
        
        # Violin plot
        violin_parts = ax3.violinplot(confidence_data, showmeans=True, showmedians=True)
        
        # Violin renklerini ayarla
        for i, pc in enumerate(violin_parts['bodies']):
            pc.set_facecolor(plt.cm.tab20(i % 20))
            pc.set_alpha(0.7)
            pc.set_edgecolor('black')
        
        # Mean ve median Ã§izgilerini renklendir
        violin_parts['cmeans'].set_color('red')
        violin_parts['cmeans'].set_linewidth(2)
        violin_parts['cmedians'].set_color('blue')
        violin_parts['cmedians'].set_linewidth(2)
        
        ax3.set_xlabel('Konular')
        ax3.set_ylabel('GÃ¼ven Skoru DaÄŸÄ±lÄ±mÄ±')
        ax3.set_title('Konu BazÄ±nda GÃ¼ven Skoru DaÄŸÄ±lÄ±mÄ± (Violin Plot)', 
                     fontsize=12, fontweight='bold')
        ax3.set_xticks(np.arange(1, len(konu_labels_violin) + 1))
        ax3.set_xticklabels(konu_labels_violin)
        ax3.set_ylim(0, 1.0)
        ax3.grid(True, alpha=0.3, axis='y')
        
        # AÃ§Ä±klama
        from matplotlib.lines import Line2D
        legend_elements_violin = [
            Line2D([0], [0], color='red', linewidth=2, label='Ortalama'),
            Line2D([0], [0], color='blue', linewidth=2, label='Medyan')
        ]
        ax3.legend(handles=legend_elements_violin, loc='upper right')
        
        plt.tight_layout()
        plt.show()
        
        # GRAFÄ°K 4: Konu gÃ¼ven Ä±sÄ± haritasÄ± (heatmap) - TEK BAÅINA
        fig4, ax4 = plt.subplots(figsize=(12, 8))
        
        # Konu gÃ¼ven matrisi oluÅŸtur
        n_topics = len(self.topics)
        confidence_matrix = np.zeros((n_topics, n_topics))
        
        for i in range(n_topics):
            for j in range(n_topics):
                if i == j:
                    # KÃ¶ÅŸegen: ortalama gÃ¼ven
                    conf_data = self.df[self.df['dominant_topic'] == i]['topic_confidence']
                    confidence_matrix[i, j] = conf_data.mean() if len(conf_data) > 0 else 0
                else:
                    # KÃ¶ÅŸegen dÄ±ÅŸÄ±: konular arasÄ± iliÅŸki (korelasyon benzeri)
                    # Ä°ki konunun birlikte gÃ¶rÃ¼lme sÄ±klÄ±ÄŸÄ±
                    doc_count_i = (self.df['dominant_topic'] == i).sum()
                    doc_count_j = (self.df['dominant_topic'] == j).sum()
                    if doc_count_i > 0 and doc_count_j > 0:
                        # Jaccard benzerliÄŸi
                        intersection = ((self.df['dominant_topic'] == i) & (self.df['dominant_topic'] == j)).sum()
                        union = doc_count_i + doc_count_j - intersection
                        if union > 0:
                            confidence_matrix[i, j] = intersection / union
        
        # Heatmap
        im = ax4.imshow(confidence_matrix, cmap='YlOrRd', vmin=0, vmax=1)
        
        # Konu etiketleri
        topic_labels_short = [f"K{i+1}" for i in range(n_topics)]
        
        # HÃ¼cre deÄŸerlerini ekle
        for i in range(n_topics):
            for j in range(n_topics):
                text = ax4.text(j, i, f"{confidence_matrix[i, j]:.2f}",
                              ha="center", va="center", 
                              color="black" if confidence_matrix[i, j] < 0.5 else "white",
                              fontsize=9, fontweight='bold')
        
        ax4.set_title('Konu GÃ¼ven Matrisi - IsÄ± HaritasÄ±', fontsize=12, fontweight='bold')
        ax4.set_xlabel('Konular')
        ax4.set_ylabel('Konular')
        ax4.set_xticks(np.arange(n_topics))
        ax4.set_yticks(np.arange(n_topics))
        ax4.set_xticklabels(topic_labels_short)
        ax4.set_yticklabels(topic_labels_short)
        
        # Renk bar'Ä±nÄ± ekle
        cbar = ax4.figure.colorbar(im, ax=ax4)
        cbar.ax.set_ylabel('GÃ¼ven / Benzerlik DeÄŸeri', rotation=270, labelpad=15)
        
        plt.tight_layout()
        plt.show()
        
        # Ä°statistiksel Ã¶zet
        print("\nğŸ“ˆ KONU GÃœVEN Ä°STATÄ°STÄ°KLERÄ°:")
        print("-" * 50)
        
        overall_mean = stats_df['mean_confidence'].mean()
        overall_std = stats_df['mean_confidence'].std()
        
        print(f"â€¢ Genel ortalama gÃ¼ven: {overall_mean:.3f} Â± {overall_std:.3f}")
        print(f"â€¢ En yÃ¼ksek gÃ¼ven: Konu {int(stats_df.loc[stats_df['mean_confidence'].idxmax(), 'topic_id'])+1} "
              f"({stats_df['mean_confidence'].max():.3f})")
        print(f"â€¢ En dÃ¼ÅŸÃ¼k gÃ¼ven: Konu {int(stats_df.loc[stats_df['mean_confidence'].idxmin(), 'topic_id'])+1} "
              f"({stats_df['mean_confidence'].min():.3f})")
        print(f"â€¢ GÃ¼ven aralÄ±ÄŸÄ± geniÅŸliÄŸi: {stats_df['mean_confidence'].max() - stats_df['mean_confidence'].min():.3f}")
        
        print(f"\nğŸ“Š GÃœVEN SEVÄ°YELERÄ° DAÄILIMI:")
        for level in ["Ã‡OK YÃœKSEK", "YÃœKSEK", "ORTA", "DÃœÅÃœK"]:
            count = (stats_df['confidence_level'] == level).sum()
            if count > 0:
                percentage = (count / len(stats_df)) * 100
                print(f"  â€¢ {level}: {count} konu (%{percentage:.1f})")
        
        return stats_df
    
    def print_istatistikler_gelistirilmis(self):
        """GeliÅŸtirilmiÅŸ istatistikler"""
        print(f"\nğŸ“Š GELÄ°ÅTÄ°RÄ°LMÄ°Å Ä°STATÄ°STÄ°KLER (Seed: {self.random_seed}):")
        print("="*70)
        
        avg_confidence = self.df['topic_confidence'].mean() * 100
        
        print(f"\nğŸ“ˆ GENEL PERFORMANS:")
        print(f"  â€¢ Ortalama gÃ¼ven: %{avg_confidence:.1f}")
        print(f"  â€¢ Toplam konuÅŸma: {len(self.df)}")
        print(f"  â€¢ Konu sayÄ±sÄ±: {len(self.topics)}")
        
        # Benzersiz etiket kontrolÃ¼
        unique_labels = set()
        duplicate_labels = []
        
        for topic in self.topics:
            if topic['label'] in unique_labels:
                duplicate_labels.append(topic['label'])
            unique_labels.add(topic['label'])
        
        print(f"\nğŸ¯ KONU Ã‡EÅÄ°TLÄ°LÄ°ÄÄ°:")
        print(f"  â€¢ Benzersiz etiket: {len(unique_labels)}/{len(self.topics)}")
        if len(unique_labels) == len(self.topics):
            print("  âœ… MÃœKEMMEL: TÃ¼m konular benzersiz etiketlere sahip!")
        else:
            print(f"  âš ï¸  UYARI: {len(duplicate_labels)} konu aynÄ± etiketi paylaÅŸÄ±yor")
            for dup in set(duplicate_labels):
                print(f"     - '{dup}'")
        
        # Konu bazlÄ± istatistikler
        print(f"\nğŸ“‹ KONU BAZINDA DAÄILIM:")
        print("-" * 70)
        
        topic_stats = []
        for topic in self.topics:
            doc_count = (self.df['dominant_topic'] == topic['id']).sum()
            if doc_count > 0:
                topic_docs = self.df[self.df['dominant_topic'] == topic['id']]
                avg_conf = topic_docs['topic_confidence'].mean() * 100
                percentage = (doc_count / len(self.df)) * 100
                
                topic_stats.append({
                    'Konu': topic['id'] + 1,
                    'Etiket': topic['label'][:40] + ('...' if len(topic['label']) > 40 else ''),
                    'DokÃ¼man': doc_count,
                    '%': f"{percentage:.1f}",
                    'Ort. GÃ¼ven': f"{avg_conf:.1f}%",
                    'Anahtar Kelimeler': ', '.join(topic['top_keywords'])
                })
        
        # Tablo olarak gÃ¶ster
        stats_df = pd.DataFrame(topic_stats)
        if not stats_df.empty:
            print("\n" + stats_df.to_string(index=False))
        
        # GÃ¼ven aralÄ±klarÄ± istatistikleri
        if hasattr(self, 'topic_confidence_stats') and self.topic_confidence_stats:
            print(f"\nğŸ“Š KONU GÃœVEN ARALIKLARI Ã–ZETÄ°:")
            print("-" * 50)
            
            conf_stats_df = pd.DataFrame(self.topic_confidence_stats)
            
            for _, row in conf_stats_df.iterrows():
                print(f"\nâ€¢ Konu {int(row['topic_id'])+1}: {row['topic_label'][:40]}...")
                print(f"  â†’ Belge sayÄ±sÄ±: {row['n_documents']}")
                print(f"  â†’ Ortalama gÃ¼ven: {row['mean_confidence']:.3f}")
                print(f"  â†’ %95 GÃ¼ven AralÄ±ÄŸÄ±: [{row['ci_lower']:.3f}, {row['ci_upper']:.3f}]")
                print(f"  â†’ Hata payÄ±: Â±{row['margin_of_error']:.3f}")
                print(f"  â†’ Seviye: {row['confidence_level']} {row['level_color']}")
    
    def run_gelistirilmis_analiz(self):
        """GeliÅŸtirilmiÅŸ analizi Ã§alÄ±ÅŸtÄ±r"""
        print("\n" + "="*70)
        print("ğŸš€ GELÄ°ÅTÄ°RÄ°LMÄ°Å LDA ANALÄ°ZÄ° BAÅLATILIYOR")
        print("="*70)
        
        try:
            # Konu sayÄ±sÄ±
            if self.n_topics is None:
                n_topics = 5  # VarsayÄ±lan
            else:
                n_topics = self.n_topics
            
            print(f"âœ… Konu sayÄ±sÄ±: {n_topics}")
            
            # DTM oluÅŸtur
            self.create_dtm_gelistirilmis(n_topics=n_topics)
            
            # LDA eÄŸitimi
            self.perform_lda_gelistirilmis(n_topics=n_topics)
            
            # Zaman analizi grafikleri (ALT ALTA)
            self.zaman_analizi_grafikleri()
            
            # Konu bazÄ±nda gÃ¼ven aralÄ±klarÄ± grafikleri
            self.konu_bazinda_guven_grafikleri()
            
            # Ä°statistikler
            self.print_istatistikler_gelistirilmis()
            
            # SonuÃ§larÄ± kaydet
            self.save_results_gelistirilmis()
            
            print("\n" + "="*70)
            print(f"âœ… GELÄ°ÅTÄ°RÄ°LMÄ°Å ANALÄ°Z BAÅARIYLA TAMAMLANDI!")
            print("="*70)
            
            return {
                'success': True,
                'topics': self.topics,
                'avg_confidence': self.df['topic_confidence'].mean(),
                'random_seed': self.random_seed,
                'n_topics': n_topics
            }
            
        except Exception as e:
            print(f"\nâŒ HATA: {e}")
            import traceback
            traceback.print_exc()
            return {'success': False, 'error': str(e)}
    
    def save_results_gelistirilmis(self):
        """SonuÃ§larÄ± kaydet"""
        output_dir = f'gelistirilmis_lda_results_seed_{self.random_seed}'
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nğŸ’¾ SONUÃ‡LAR '{output_dir}' KLASÃ–RÃœNE KAYDEDÄ°LÄ°YOR...")
        
        # TÃ¼m sonuÃ§larÄ± kaydet
        self.df.to_csv(f'{output_dir}/tum_sonuclar.csv', index=False, encoding='utf-8-sig')
        print(f"âœ“ TÃ¼m sonuÃ§lar kaydedildi: {output_dir}/tum_sonuclar.csv")
        
        # Konu Ã¶zeti
        summary_data = []
        for topic in self.topics:
            doc_count = (self.df['dominant_topic'] == topic['id']).sum()
            topic_docs = self.df[self.df['dominant_topic'] == topic['id']]
            
            summary_data.append({
                'konu_no': topic['id'] + 1,
                'konu_etiketi': topic['label'],
                'dokuman_sayisi': doc_count,
                'yuzde': (doc_count / len(self.df)) * 100,
                'ortalama_guven': topic_docs['topic_confidence'].mean() * 100,
                'anahtar_kelimeler': ', '.join(topic['keywords'][:10])
            })
        
        pd.DataFrame(summary_data).to_csv(
            f'{output_dir}/konu_ozeti.csv', 
            index=False, encoding='utf-8-sig'
        )
        print(f"âœ“ Konu Ã¶zeti kaydedildi: {output_dir}/konu_ozeti.csv")
        
        # GÃ¼ven aralÄ±klarÄ±
        if hasattr(self, 'topic_confidence_stats') and self.topic_confidence_stats:
            conf_df = pd.DataFrame(self.topic_confidence_stats)
            conf_df.to_csv(
                f'{output_dir}/guven_araliklari.csv',
                index=False, encoding='utf-8-sig'
            )
            print(f"âœ“ GÃ¼ven aralÄ±klarÄ± kaydedildi: {output_dir}/guven_araliklari.csv")
        
        print(f"\nğŸ“ TÃœM SONUÃ‡LAR: {os.path.abspath(output_dir)}/")


# ============================================================================
# ANA PROGRAM
# ============================================================================

def main_gelistirilmis():
    """Ana program"""
    
    CSV_PATH = "smart_stopwords_results/filtered_speeches.csv"
    
    if not os.path.exists(CSV_PATH):
        print(f"âŒ Dosya bulunamadÄ±: {CSV_PATH}")
        return
    
    print("\nğŸ¯ GELÄ°ÅTÄ°RÄ°LMÄ°Å PUTÄ°N KONUÅMALARI ANALÄ°ZÄ°")
    print("="*70)
    
    # Random seed
    seed_input = input("Random seed girin (boÅŸ=42): ").strip()
    RANDOM_SEED = int(seed_input) if seed_input else 42
    
    # Konu sayÄ±sÄ±
    topics_input = input("Konu sayÄ±sÄ± girin (2-8, boÅŸ=5): ").strip()
    if topics_input:
        N_TOPICS = int(topics_input)
        if N_TOPICS < 2 or N_TOPICS > 8:
            print("âš ï¸  2-8 arasÄ± olmalÄ±, varsayÄ±lan 5 kullanÄ±lÄ±yor")
            N_TOPICS = 5
    else:
        N_TOPICS = 5
    
    print(f"\nâœ… AYARLAR:")
    print(f"  â€¢ Random seed: {RANDOM_SEED}")
    print(f"  â€¢ Konu sayÄ±sÄ±: {N_TOPICS}")
    print(f"  â€¢ Zaman analizi: AKTÄ°F")
    print(f"  â€¢ Ã–nemli olay iÅŸaretleyicileri: AKTÄ°F (9 Ã¶nemli olay)")
    print(f"  â€¢ Konu gÃ¼ven aralÄ±klarÄ±: AKTÄ°F (4 yeni grafik)")
    print(f"  â€¢ GÃ¼ncellenmiÅŸ stopwords: AKTÄ°F")
    
    input(f"\nâ Analizi baÅŸlatmak iÃ§in ENTER'a basÄ±n (Seed: {RANDOM_SEED})...")
    
    analyzer = PutinLDAGelistirilmis(CSV_PATH, random_seed=RANDOM_SEED, n_topics=N_TOPICS)
    results = analyzer.run_gelistirilmis_analiz()
    
    if results.get('success', False):
        print(f"\nâœ¨ ANALÄ°Z TAMAMLANDI!")
        print(f"  â€¢ Konu sayÄ±sÄ±: {results['n_topics']}")
        print(f"  â€¢ Ortalama gÃ¼ven: %{results['avg_confidence']*100:.1f}")
        print(f"  â€¢ Zaman analizi grafikleri: 4 GRAFÄ°K OLUÅTURULDU")
        print(f"  â€¢ Konu gÃ¼ven grafikleri: 4 GRAFÄ°K OLUÅTURULDU")
        print(f"  â€¢ Ã–nemli olaylar: 9 olay iÅŸaretlendi")
        
        # Benzersiz etiket kontrolÃ¼
        unique_labels = set(t['label'] for t in results['topics'])
        if len(unique_labels) == len(results['topics']):
            print(f"  âœ… KONU ETÄ°KETLERÄ°: {len(unique_labels)}/{len(results['topics'])} benzersiz")
        else:
            print(f"  âš ï¸  KONU ETÄ°KETLERÄ°: {len(unique_labels)}/{len(results['topics'])} benzersiz")
        
        print(f"\nğŸ“ SONUÃ‡LAR: 'gelistirilmis_lda_results_seed_{RANDOM_SEED}/' klasÃ¶rÃ¼nde")
    
    else:
        print(f"\nâŒ HATA: {results.get('error', 'Bilinmeyen')}")


if __name__ == "__main__":
    main_gelistirilmis()


